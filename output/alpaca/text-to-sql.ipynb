{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d598c9c-ff35-4013-9aa5-5036b46cccbc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d620d-29a1-4fc3-ab6e-7bde2d6466b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "Training Alpaca-LoRA model with params:\n",
      "base_model: ../llama-7b-hf\n",
      "data_path: ../sql-train.jsonl\n",
      "output_dir: ./lora-alpaca\n",
      "batch_size: 128\n",
      "micro_batch_size: 4\n",
      "num_epochs: 3\n",
      "learning_rate: 0.0001\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:11<00:00,  2.89it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Using custom data configuration default-e235e3b2b9041f2d\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e235e3b2b9041f2d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 380.75it/s]\n",
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-e235e3b2b9041f2d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-99c37b9b7ec03f75.arrow and /root/.cache/huggingface/datasets/json/default-e235e3b2b9041f2d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0034a5fab6830b22.arrow\n",
      "100%|███████████████████████████████████| 54355/54355 [00:31<00:00, 1702.08ex/s]\n",
      "100%|█████████████████████████████████████| 2000/2000 [00:01<00:00, 1731.36ex/s]\n",
      "{'loss': 2.9686, 'learning_rate': 1e-05, 'epoch': 0.02}                         \n",
      "  1%|▌                                      | 17/1272 [06:39<8:05:12, 23.20s/it]"
     ]
    }
   ],
   "source": [
    "!cd alpaca-lora && python finetune.py \\\n",
    "    --base_model '../llama-7b-hf' \\\n",
    "    --data_path '../sql-train.jsonl' \\\n",
    "    --output_dir './lora-alpaca' \\\n",
    "    --batch_size 128 \\\n",
    "    --micro_batch_size 4 \\\n",
    "    --num_epochs 3 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --val_set_size 2000 \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --lora_target_modules '[q_proj,v_proj]' \\\n",
    "    --train_on_inputs \\\n",
    "    --group_by_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077ac84-de1c-4b72-a1e1-b160fdb768e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
