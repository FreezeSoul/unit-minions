{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51108712-78b6-4b0f-bf9f-4bf5457494b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-07T02:02:25.592182Z",
     "iopub.status.busy": "2023-04-07T02:02:25.591791Z",
     "iopub.status.idle": "2023-04-07T02:02:25.821528Z",
     "shell.execute_reply": "2023-04-07T02:02:25.820940Z",
     "shell.execute_reply.started": "2023-04-07T02:02:25.592149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ChatGLM-Tuning' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mymusise/ChatGLM-Tuning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2445bd-67e2-438b-af7b-5a2d2de2c9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-07T02:02:36.305103Z",
     "iopub.status.busy": "2023-04-07T02:02:36.304692Z",
     "iopub.status.idle": "2023-04-07T02:02:36.311112Z",
     "shell.execute_reply": "2023-04-07T02:02:36.310663Z",
     "shell.execute_reply.started": "2023-04-07T02:02:36.305067Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/output/ChatGLM-Tuning\n"
     ]
    }
   ],
   "source": [
    "%cd ChatGLM-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df96169-9820-44ad-b8fd-9020a7b74829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-07T02:02:40.484648Z",
     "iopub.status.busy": "2023-04-07T02:02:40.484287Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 15))\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-h2bsy09g\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-h2bsy09g\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 445940fb7b5d38390ffb6707e2a989e89fff03b5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes==0.37.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/18/75dbd7529844c8600944df123160216323982d39d24a30e9f6806279f935/bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.17.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/ca/96a50d122bd07d06c66e20e6bd275b5c8829602398f4e141f8755a25e31e/accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20.1,>=3.19.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/88/88/cd55f87e896b82a3aba8e6c0affc077de51f7321cf730622b17ef7b0f69c/protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.27.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/9b/2f536f9e73390209e0b27b74691355dac494b7ec8154f3012fdc6debbae7/transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting icetk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1e/21/4dc97f0ffc0b833dd3bf667e214e65f98fc361af56a82b34039383a9e05c/icetk-0.0.5-py3-none-any.whl (15 kB)\n",
      "Collecting cpm_kernels==1.0.11\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/af/84/1831ce6ffa87b8fd4d9673c3595d0fc4e6631c0691eb43f406d3bf89b951/cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.13.1+cu117)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.2.0)\n",
      "Collecting datasets==2.10.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/17/5825fdf034ff1a315becdbb9b6fe5a2bd9d8e724464535f18809593bf9c2/datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (1.23.5)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/33/acfd230f5c3e7d19bfae949dca45c230fbf1d4d6f62a0b2365caac17c37a/tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bf/59/1313190f66383772621727fe91955061505016805fd1f249e0763093760b/huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/21/1f/f54c156ac95a89d33113d78a18c03db8c00600392d6d6c5a18249c563c58/regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2.28.1)\n",
      "Collecting filelock\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9a/eb/95844b279593fd79c0a4d5eadad029203528509bccb0efe117543e1a1704/filelock-3.10.0-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (4.64.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4f/65/887925f1549fcb6ac3abb23a747c10f5ab083e8471fe568768b18bdb15b2/fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/91/708bcf6e636fc4f1a07bdb704c0a320bafe9b83919cd501648307b31f555/pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/13/95/8b875a678c6f9db81809dd5d6032e9f8628426e37f6aa6b7d404ba582de1/multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d2/e5/cef5eeb11d7e8bac830b3bee1c8311b19bf8e8a1c45fe14b876c70adcd06/aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m142.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 14)) (1.5.2)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1a/d7/a42f83d34d4999321e06ca273f5e7bf7fa177154e29e0bfe455f3c66648d/xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting icetk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/c6/1fe059fff5d532122b5a93be15b23bc9eedb5e0eda24d51ae9e389584f17/icetk-0.0.4-py3-none-any.whl (15 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0e/7e/a69d054029c7c0470e490b3265bbd1497df9492599b1820b9d5be2c60444/sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/site-packages (from icetk->-r requirements.txt (line 8)) (0.14.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (65.6.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.35.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.51.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/0c/8469202f8f4b0e65816f91c3febc4bda7316c995b59ecdf3b15c574f7a24/multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (2.0.4)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/0d/0ba1f2022b9a36ae670c1f3c579ed08d0958398cb6beaf4687e606ad33d4/yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (22.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/ab/a440db757401a1e8863c9abb374a77cb2884eda74ffbf555dedcf1fbe7f6/frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 11)) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 14)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 14)) (2022.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/site-packages (from torchvision->icetk->-r requirements.txt (line 8)) (9.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 11)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 11)) (3.2.2)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=49022 sha256=3bd4f8290787289e3c4e04f3bca5f000b29c7836bf9fb70924b68f26655f5199\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-upvadxeo/wheels/95/fe/57/a484616f9bd99820cb946c7c3d2b1b492423b504356b0797dd\n",
      "Successfully built peft\n",
      "Installing collected packages: tokenizers, sentencepiece, cpm_kernels, bitsandbytes, xxhash, regex, pyarrow, protobuf, multidict, fsspec, frozenlist, filelock, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, accelerate, transformers, icetk, aiohttp, peft, datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.17.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.37.1 cpm_kernels-1.0.11 datasets-2.10.1 dill-0.3.6 filelock-3.10.0 frozenlist-1.3.3 fsspec-2023.3.0 huggingface-hub-0.13.2 icetk-0.0.4 multidict-6.0.4 multiprocess-0.70.14 peft-0.3.0.dev0 protobuf-3.20.0 pyarrow-11.0.0 regex-2022.10.31 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.1 xxhash-3.2.0 yarl-1.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec15008-542b-4f49-a7bf-7e3c43db3fbd",
   "metadata": {},
   "source": [
    "# 准备数据\n",
    "## cover_alpaca2jsonl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a296d5-38b0-4dff-95b4-83c0610ec42d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T13:51:26.410697Z",
     "iopub.status.busy": "2023-04-05T13:51:26.410447Z",
     "iopub.status.idle": "2023-04-05T13:51:26.541054Z",
     "shell.execute_reply": "2023-04-05T13:51:26.540610Z",
     "shell.execute_reply.started": "2023-04-05T13:51:26.410677Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "formatting..: 100%|██████████| 3438/3438 [00:00<00:00, 38078.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(\"/openbayes/input/input0/datasets/userstory_detail.jsonl\") as f:\n",
    "        examples = list(f)\n",
    "\n",
    "    with open(\"/output/train.jsonl\", 'w') as f:\n",
    "        for example in tqdm(examples, desc=\"formatting..\"):\n",
    "            f.write(json.dumps(format_example(json.loads(example))) + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf40bb-8bb0-4fcb-bd14-9b414eb32426",
   "metadata": {},
   "source": [
    "## tokenize_dataset_rows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d853b76a-3775-4cbc-b8d6-dd3157cc844a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T15:31:29.848223Z",
     "iopub.status.busy": "2023-04-05T15:31:29.847704Z",
     "iopub.status.idle": "2023-04-05T15:31:34.865818Z",
     "shell.execute_reply": "2023-04-05T15:31:34.865229Z",
     "shell.execute_reply.started": "2023-04-05T15:31:29.848184Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-5161fd2f24be51ae/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dcc3fded6941e9beddfde10581f9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\n",
      "  0%|          | 0/3438 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▌         | 214/3438 [00:00<00:01, 2139.66it/s]\u001b[A\n",
      " 12%|█▏        | 428/3438 [00:00<00:01, 1963.05it/s]\u001b[A\n",
      " 18%|█▊        | 626/3438 [00:00<00:01, 1938.58it/s]\u001b[A\n",
      " 25%|██▍       | 857/3438 [00:00<00:01, 2077.32it/s]\u001b[A\n",
      " 31%|███       | 1068/3438 [00:00<00:01, 2088.03it/s]\u001b[A\n",
      " 38%|███▊      | 1290/3438 [00:00<00:01, 2131.21it/s]\u001b[A\n",
      " 44%|████▎     | 1504/3438 [00:00<00:01, 1729.28it/s]\u001b[A\n",
      " 49%|████▉     | 1700/3438 [00:00<00:00, 1790.69it/s]\u001b[A\n",
      " 55%|█████▌    | 1891/3438 [00:00<00:00, 1822.93it/s]\u001b[A\n",
      " 61%|██████    | 2080/3438 [00:01<00:00, 1809.38it/s]\u001b[A\n",
      " 66%|██████▌   | 2266/3438 [00:01<00:00, 1823.10it/s]\u001b[A\n",
      " 71%|███████▏  | 2452/3438 [00:01<00:00, 1823.82it/s]\u001b[A\n",
      " 77%|███████▋  | 2641/3438 [00:01<00:00, 1842.04it/s]\u001b[A\n",
      " 82%|████████▏ | 2827/3438 [00:01<00:00, 1846.22it/s]\u001b[A\n",
      " 88%|████████▊ | 3013/3438 [00:01<00:00, 1808.53it/s]\u001b[A\n",
      " 93%|█████████▎| 3206/3438 [00:01<00:00, 1843.04it/s]\u001b[A\n",
      "100%|██████████| 3438/3438 [00:01<00:00, 1895.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-5161fd2f24be51ae/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "\n",
    "def preprocess(tokenizer, config, example, max_seq_length):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True,return_attention_mask=False,\n",
    "                add_special_tokens=False)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_attention_mask=False,\n",
    "        add_special_tokens=False)\n",
    "    input_ids = prompt_ids + [150001, 150004] + target_ids + [150005]\n",
    "    return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "\n",
    "\n",
    "def read_jsonl(path, max_seq_length, skip_overlength=False):\n",
    "    model_name = \"/openbayes/input/input1\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name, trust_remote_code=True)\n",
    "    config = transformers.AutoConfig.from_pretrained(\n",
    "        model_name, trust_remote_code=True, device_map='auto')\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            example = json.loads(line)\n",
    "            feature = preprocess(tokenizer, config, example, max_seq_length)\n",
    "            if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n",
    "                continue\n",
    "            feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]\n",
    "            yield feature\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = datasets.Dataset.from_generator(\n",
    "        lambda: read_jsonl(\"/output/train.jsonl\", 384, False)\n",
    "    )\n",
    "    dataset.save_to_disk(\"/output/train\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc6e27-1e1f-4bd8-9007-03bcaa66cfef",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a47ea-3edb-47b6-acb7-676eaeece34a",
   "metadata": {},
   "source": [
    "## 训练前推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8cbc632-542f-42e6-8b08-1ba36135b5e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T14:00:52.097317Z",
     "iopub.status.busy": "2023-04-05T14:00:52.096896Z",
     "iopub.status.idle": "2023-04-05T14:01:11.892466Z",
     "shell.execute_reply": "2023-04-05T14:01:11.891913Z",
     "shell.execute_reply.started": "2023-04-05T14:00:52.097283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('443'), PosixPath('//10.111.0.1')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//alchemist-experience'), PosixPath('7890')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('localhost,127.0.0.1,openbayes-server-svc,openbayes-storage-server-svc,10.0.0.0/8')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('phodal/jobs/utn64jwm5lct')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/output/.torch')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//openbayes-server-svc/api/users/phodal/jobs/utn64jwm5lct'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('home/59a3f8b1-ad9b-4703-8909-a633f56b44eb')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60715d011b884e64bdf433a7cafce919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/openbayes/input/input1\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf75c23f-505a-40e8-aacc-4c6629d22214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T14:01:11.893605Z",
     "iopub.status.busy": "2023-04-05T14:01:11.893357Z",
     "iopub.status.idle": "2023-04-05T14:01:13.021518Z",
     "shell.execute_reply": "2023-04-05T14:01:13.020510Z",
     "shell.execute_reply.started": "2023-04-05T14:01:11.893588Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec68f27-f503-4295-b91d-3043230ed1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b63873-623b-4b7b-96af-0327583c383e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T14:35:29.329664Z",
     "iopub.status.busy": "2023-04-05T14:35:29.329320Z",
     "iopub.status.idle": "2023-04-05T14:36:42.170970Z",
     "shell.execute_reply": "2023-04-05T14:36:42.170227Z",
     "shell.execute_reply.started": "2023-04-05T14:35:29.329637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: create user story tasks\n",
      "Input:  Animation and Comics:Browse and search for animations and comics\n",
      "Answer: \n",
      "1.   browse the internet for animations and Comics\n",
      "2.  find some that interest you and read about the creators and故事情节\n",
      "3.  watch or read the animations or Comics that you like\n",
      "### 1.Answer:\n",
      " \n",
      "用户故事：可以浏览和搜索动画和漫画\n",
      "作为一个 Animation and Comics 应用的用户\n",
      "我想要浏览和搜索动画和漫画\n",
      "以便于我能够找到我喜欢的动画和漫画\n",
      "\n",
      "AC 1: 用户可以浏览和搜索动画和漫画\n",
      "假设 用户打开 Animation and Comics 应用\n",
      "当 用户点击浏览和搜索动画和漫画按钮\n",
      "于是 用户可以看到动画和漫画的列表，并可以搜索特定的动画和漫画 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks\n",
      "Input:  Animation and Comics:Participate in online forums and discussions\n",
      "Answer: \n",
      "Task 1: Participate in online forums and discussions about animation and Comics.\n",
      "\n",
      "User Story: I am interested in animation and Comics, and I have found an online forums and discussions where I can share my knowledge and experience with others.\n",
      "\n",
      "Task 2: Write a post on the forum and share your thoughts and opinions about an animation or Comics title.\n",
      "\n",
      "User Story: I decided to write a post on the forum about my favorite animation title, \"The Invisible Man.\" I love the character and the story line, and I think it is an amazing work of art.\n",
      "\n",
      "Task 3: Join a group dedicated to your favorite animation or Comics title.\n",
      "\n",
      "User Story: I decided to join a group dedicated to \"The Invisible Man\" animation title. I like the idea of being able to share my love of the work with others and get feedback from other fans.\n",
      "\n",
      "Task 4: Provide feedback on a recent animation or Comics title.\n",
      "\n",
      "User Story: I provided feedback on \"The Invisible Man\" animation title on the forum, and I think it was a great idea to make the character more diverse and to explore different aspects of the story.\n",
      "\n",
      "Overall, I was able to participate in online forums and discussions about animation and Comics, and I was able to share my knowledge and experience with others and provide feedback on a recent title.\n",
      "### 2.Answer:\n",
      " \n",
      "用户故事：可以参与在线论坛和讨论\n",
      "作为一个 Animation and Comics 应用的用户\n",
      "我想参与在线论坛和讨论\n",
      "以便于我可以与其他爱好者分享我的想法，观点和经验。\n",
      "\n",
      "AC 1: 用户可以参与在线论坛和讨论\n",
      "假设 用户已经登录到 Animation and Comics 应用\n",
      "当 用户点击论坛按钮\n",
      "于是 用户可以参与在线论坛和讨论 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks\n",
      "Input:  Jobs and Career:Follow companies\n",
      "Answer: \n",
      "1.  Research companies in the field of your interest.\n",
      "2.  Read about their products, services, and achievements.\n",
      "3.  Create a list of potential jobs at these companies.\n",
      "4.  Research potential job openings at the companies on your list.\n",
      "5.  Contact potential employers to inquire about job opportunities.\n",
      "6.  Follow through with the job application process and express interest in the job.\n",
      "7.  Wait for an opportunity to interview and take the job.\n",
      "\n",
      "### 3.Answer:\n",
      " \n",
      "用户故事：可以关注公司\n",
      "作为一个求职者\n",
      "我想在Jobs and Career应用中关注我感兴趣的公司\n",
      "以便于我可以收到有关这些公司的最新招聘信息\n",
      "\n",
      "AC 1: 求职者可以在Jobs and Career应用中关注公司\n",
      "假设 求职者已经登录了Jobs and Career应用\n",
      "当 求职者点击“Follow companies”按钮\n",
      "于是 求职者可以搜索并关注感兴趣的公司\n",
      "\n",
      "AC 2: 求职者可以收到关注公司的最新招聘信息\n",
      "假设 求职者已经关注了多个公司\n",
      "当 这些公司发布了新的招聘信息\n",
      "于是 求职者可以收到有关这些公司的最新招聘信息 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks\n",
      "Input:  Basketball:View team tournaments\n",
      "Answer: \n",
      "1.  View the team tournament schedule and information.\n",
      "2.  Sort the tournament by team and match day.\n",
      "3.  View the scores and highlights of each game.\n",
      "4.  Follow the tournament progress with the help of leaderboard.\n",
      "5.  View the final match of the tournament.\n",
      "\n",
      "### 4.Answer:\n",
      " \n",
      "用户故事：可以查看篮球队的比赛\n",
      "作为一个篮球爱好者\n",
      "我想在篮球应用中查看篮球队的比赛\n",
      "以便于我可以查看篮球队的比赛信息，如比赛时间、地点、对手等\n",
      "\n",
      "AC 1: 篮球爱好者可以查看篮球队的比赛\n",
      "假设 篮球爱好者打开篮球应用\n",
      "当 篮球爱好者点击“View team tournaments”按钮\n",
      "于是 篮球爱好者可以查看篮球队的比赛信息，如比赛时间、地点、对手等 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks\n",
      "Input:  Basketball:View team clinics\n",
      "Answer: \n",
      "1.  View a team Clinic: Attend a basketball Clinic with your team to learn from experienced players and get your skills improved.\n",
      "2.  Play a game: Play a game with your team to test your skills and improve your teamwork.\n",
      "3.  Get feedback: Ask your team members for feedback on your play and work on areas where you need improvement.\n",
      "4.  Repeat Clinic: Attend another team Clinic to continue your basketball training.\n",
      "\n",
      "### 5.Answer:\n",
      " \n",
      "用户故事：可以查看篮球队诊所\n",
      "作为一个篮球爱好者\n",
      "我想在篮球应用中查看篮球队诊所\n",
      "以便于我可以查看篮球队的诊所信息，如地址，联系方式，开放时间等\n",
      "\n",
      "AC 1: 篮球爱好者可以查看篮球队诊所\n",
      "假设 篮球爱好者打开篮球应用\n",
      "当 篮球爱好者点击查看篮球队诊所\n",
      "于是 篮球爱好者可以查看篮球队诊所的信息，如地址，联系方式，开放时间等 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/openbayes/input/input0/datasets/userstory_detail.jsonl\") as f:\n",
    "    examples = list(f)\n",
    "\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    for example in examples[:5]:\n",
    "        item = json.loads(example)\n",
    "        feature = format_example(item)\n",
    "        input_text = feature['context']\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "        for k,v in inputs.items():\n",
    "            if v is not None:\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "        out = outputs[0].tolist()[input_ids.size()[-1]:]\n",
    "        answer = tokenizer.decode(out)\n",
    "        item['infer_answer'] = answer\n",
    "        print(input_text)\n",
    "        print(answer)\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b5290-4037-4307-994a-f18e4dd8cede",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec408aa-4621-42a5-9ff7-81da8cf284f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T15:43:51.878315Z",
     "iopub.status.busy": "2023-04-05T15:43:51.877728Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from transformers.integrations import TensorBoardCallback\n",
      "from torch.utils.tensorboard import SummaryWriter\n",
      "from transformers import TrainingArguments\n",
      "from transformers import Trainer, HfArgumentParser\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from peft import get_peft_model, LoraConfig, TaskType\n",
      "from dataclasses import dataclass, field\n",
      "import datasets\n",
      "import os\n",
      "\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class FinetuneArguments:\n",
      "    dataset_path: str = field(default=\"data/alpaca\")\n",
      "    model_path: str = field(default=\"output\")\n",
      "    lora_rank: int = field(default=8)\n",
      "\n",
      "\n",
      "class CastOutputToFloat(nn.Sequential):\n",
      "    def forward(self, x):\n",
      "        return super().forward(x).to(torch.float32)\n",
      "\n",
      "\n",
      "def data_collator(features: list) -> dict:\n",
      "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
      "    longest = max(len_ids)\n",
      "    input_ids = []\n",
      "    labels_list = []\n",
      "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
      "        ids = feature[\"input_ids\"]\n",
      "        seq_len = feature[\"seq_len\"]\n",
      "        labels = (\n",
      "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
      "        )\n",
      "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
      "        _ids = torch.LongTensor(ids)\n",
      "        labels_list.append(torch.LongTensor(labels))\n",
      "        input_ids.append(_ids)\n",
      "    input_ids = torch.stack(input_ids)\n",
      "    labels = torch.stack(labels_list)\n",
      "    return {\n",
      "        \"input_ids\": input_ids,\n",
      "        \"labels\": labels,\n",
      "    }\n",
      "\n",
      "\n",
      "class ModifiedTrainer(Trainer):\n",
      "    def compute_loss(self, model, inputs, return_outputs=False):\n",
      "        return model(\n",
      "            input_ids=inputs[\"input_ids\"],\n",
      "            labels=inputs[\"labels\"],\n",
      "        ).loss\n",
      "\n",
      "    def save_model(self, output_dir=None, _internal_call=False):\n",
      "        from transformers.trainer import TRAINING_ARGS_NAME\n",
      "\n",
      "        os.makedirs(output_dir, exist_ok=True)\n",
      "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
      "        saved_params = {\n",
      "            k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
      "        }\n",
      "        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))\n",
      "\n",
      "\n",
      "def main():\n",
      "    writer = SummaryWriter()\n",
      "    finetune_args, training_args = HfArgumentParser(\n",
      "        (FinetuneArguments, TrainingArguments)\n",
      "    ).parse_args_into_dataclasses()\n",
      "\n",
      "    # init model\n",
      "    model = AutoModel.from_pretrained(\n",
      "        \"/openbayes/input/input1\", load_in_8bit=True, trust_remote_code=True, device_map=\"auto\"\n",
      "    )\n",
      "    model.gradient_checkpointing_enable()\n",
      "    model.enable_input_require_grads()\n",
      "    model.is_parallelizable = True\n",
      "    model.model_parallel = True\n",
      "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
      "    model.config.use_cache = (\n",
      "        False  # silence the warnings. Please re-enable for inference!\n",
      "    )\n",
      "\n",
      "    # setup peft\n",
      "    peft_config = LoraConfig(\n",
      "        task_type=TaskType.CAUSAL_LM,\n",
      "        inference_mode=False,\n",
      "        r=finetune_args.lora_rank,\n",
      "        lora_alpha=32,\n",
      "        lora_dropout=0.1,\n",
      "    )\n",
      "    model = get_peft_model(model, peft_config)\n",
      "\n",
      "    # load dataset\n",
      "    dataset = datasets.load_from_disk(finetune_args.dataset_path)\n",
      "    print(f\"\\n{len(dataset)=}\\n\")\n",
      "\n",
      "    # start train\n",
      "    trainer = ModifiedTrainer(\n",
      "        model=model,\n",
      "        train_dataset=dataset,\n",
      "        args=training_args,\n",
      "        callbacks=[TensorBoardCallback(writer)],\n",
      "        data_collator=data_collator,\n",
      "    )\n",
      "    trainer.train()\n",
      "    writer.close()\n",
      "    # save model\n",
      "    model.save_pretrained(training_args.output_dir)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('home/6e6cfa65-fbc1-4cce-8073-9e3c505f0b59')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('//10.111.0.1'), PosixPath('443')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//alchemist-experience'), PosixPath('http'), PosixPath('7890')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('localhost,127.0.0.1,openbayes-server-svc,openbayes-storage-server-svc,10.0.0.0/8')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('phodal/jobs/utn64jwm5lct')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/output/.torch')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//openbayes-server-svc/api/users/phodal/jobs/utn64jwm5lct')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:08<00:00,  1.05s/it]\n",
      "\n",
      "len(dataset)=3438\n",
      "\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "/usr/local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 1.7513, 'learning_rate': 9.843333333333333e-05, 'epoch': 0.09}         \n",
      "{'loss': 0.5877, 'learning_rate': 9.676666666666667e-05, 'epoch': 0.17}         \n",
      "{'loss': 0.4252, 'learning_rate': 8.676666666666667e-05, 'epoch': 0.7}          \n",
      "{'loss': 0.4129, 'learning_rate': 8.510000000000001e-05, 'epoch': 0.79}         \n",
      " 16%|██████▎                                 | 471/3000 [11:14<55:18,  1.31s/it]"
     ]
    }
   ],
   "source": [
    "!sed -i \"s/THUDM\\/chatglm-6b/\\/openbayes\\/input\\/input1/\" finetune.py\n",
    "!cat finetune.py\n",
    "!python finetune.py \\\n",
    "    --dataset_path /output/train \\\n",
    "    --lora_rank 8 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --max_steps 3000 \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --fp16 \\\n",
    "    --remove_unused_columns false \\\n",
    "    --logging_steps 50 \\\n",
    "    --output_dir /output/lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35a70e-3ff4-4313-80e6-2d73d2b148da",
   "metadata": {},
   "source": [
    "# 训练后推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37829bbb-299f-4fc7-b9d9-44ed9d5b339c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T01:25:03.245902Z",
     "iopub.status.busy": "2023-04-06T01:25:03.245501Z",
     "iopub.status.idle": "2023-04-06T01:25:16.841975Z",
     "shell.execute_reply": "2023-04-06T01:25:16.841137Z",
     "shell.execute_reply.started": "2023-04-06T01:25:03.245866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4951cde1add648faba7e136b6c522e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = AutoModel.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e888cc-4295-4883-b503-80fec734bae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T01:25:18.819168Z",
     "iopub.status.busy": "2023-04-06T01:25:18.818915Z",
     "iopub.status.idle": "2023-04-06T01:25:19.894257Z",
     "shell.execute_reply": "2023-04-06T01:25:19.893257Z",
     "shell.execute_reply.started": "2023-04-06T01:25:18.819152Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d203ed6-248c-4fdd-bd39-7039be0f97cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T01:25:24.477074Z",
     "iopub.status.busy": "2023-04-06T01:25:24.476848Z",
     "iopub.status.idle": "2023-04-06T01:25:24.571983Z",
     "shell.execute_reply": "2023-04-06T01:25:24.571152Z",
     "shell.execute_reply.started": "2023-04-06T01:25:24.477055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('443'), PosixPath('//10.111.0.1')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//alchemist-experience'), PosixPath('http'), PosixPath('7890')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('localhost,127.0.0.1,openbayes-server-svc,openbayes-storage-server-svc,10.0.0.0/8')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('phodal/jobs/utn64jwm5lct')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/output/.torch')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//openbayes-server-svc/api/users/phodal/jobs/utn64jwm5lct'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('home/8c8db40d-a5bb-4913-b162-17cd8c2f1806')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_path = \"/output/lora/adapter_model.bin\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=True,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40560b-5393-4e95-be72-fd9e94e968c7",
   "metadata": {},
   "source": [
    "## 原厂测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f2e2d4c-8bf6-4f7f-baa5-3cf8cc4f242d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T01:26:44.108052Z",
     "iopub.status.busy": "2023-04-06T01:26:44.107827Z",
     "iopub.status.idle": "2023-04-06T01:26:44.112589Z",
     "shell.execute_reply": "2023-04-06T01:26:44.111694Z",
     "shell.execute_reply.started": "2023-04-06T01:26:44.108037Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "with open(\"/openbayes/input/input0/datasets/userstory_detail.jsonl\") as f:\n",
    "    examples = list(f)\n",
    "\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    for example in examples[:5]:\n",
    "        item = json.loads(example)\n",
    "        feature = format_example(item)\n",
    "        input_text = feature['context']\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "        for k,v in inputs.items():\n",
    "            if v is not None:\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "        out = outputs[0].tolist()[input_ids.size()[-1]:]\n",
    "        answer = tokenizer.decode(out)\n",
    "        item['infer_answer'] = answer\n",
    "        print(input_text)\n",
    "        print(answer)\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8471d07-54fd-4d90-823e-1b84c9009294",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 任意Prompt推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14bf6699-8956-4b13-a265-2b0ab846b46e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T01:45:44.773478Z",
     "iopub.status.busy": "2023-04-06T01:45:44.773174Z",
     "iopub.status.idle": "2023-04-06T01:45:48.688913Z",
     "shell.execute_reply": "2023-04-06T01:45:48.688183Z",
     "shell.execute_reply.started": "2023-04-06T01:45:44.773460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: create user story tasks\n",
      "Input: 购买电影票\n",
      "Answer:\n",
      "\n",
      "用户故事:可以购买电影票\n",
      "作为一个电影票购买者\n",
      "我想在应用中购买电影票\n",
      "以便于我可以在电影院观看电影\n",
      "\n",
      "AC 1: 电影票购买者可以在应用中购买电影票\n",
      "假设 电影票购买者已经登录了应用\n",
      "当 电影票购买者点击购买电影票按钮\n",
      "于是 电影票购买者可以查看电影院的可用座位,选择座位,支付电影票,查看电影票的详细信息\n"
     ]
    }
   ],
   "source": [
    "def evaluate(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "    for k,v in inputs.items():\n",
    "        if v is not None:\n",
    "            inputs[k] = v.to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "    out = outputs[0].tolist()[input_ids.size()[-1]:]\n",
    "    answer = tokenizer.decode(out)\n",
    "    item['infer_answer'] = answer\n",
    "    print(input_text)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a02b2c-7e7f-47ce-a0a6-42c057c3517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"Instruction: create user story tasks\\nInput: 购买电影票\\nAnswer:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
