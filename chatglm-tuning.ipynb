{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91975d07-bb25-46a8-9a2b-5444042e975c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51108712-78b6-4b0f-bf9f-4bf5457494b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/mymusise/ChatGLM-Tuning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2445bd-67e2-438b-af7b-5a2d2de2c9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T05:05:30.218303Z",
     "iopub.status.busy": "2023-04-08T05:05:30.217893Z",
     "iopub.status.idle": "2023-04-08T05:05:30.229047Z",
     "shell.execute_reply": "2023-04-08T05:05:30.228183Z",
     "shell.execute_reply.started": "2023-04-08T05:05:30.218267Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/output/ChatGLM-Tuning\n"
     ]
    }
   ],
   "source": [
    "%cd ChatGLM-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df96169-9820-44ad-b8fd-9020a7b74829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T05:05:48.270181Z",
     "iopub.status.busy": "2023-04-08T05:05:48.269760Z",
     "iopub.status.idle": "2023-04-08T05:07:05.440154Z",
     "shell.execute_reply": "2023-04-08T05:07:05.438856Z",
     "shell.execute_reply.started": "2023-04-08T05:05:48.270146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08 (from -r requirements.txt (line 15))\n",
      "  Cloning https://github.com/huggingface/peft.git (to revision e536616888d51b453ed354a6f1e243fecb02ea08) to /tmp/pip-req-build-_5goabvu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-_5goabvu\n",
      "  Running command git rev-parse -q --verify 'sha^e536616888d51b453ed354a6f1e243fecb02ea08'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft.git e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Running command git checkout -q e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Resolved https://github.com/huggingface/peft.git to commit e536616888d51b453ed354a6f1e243fecb02ea08\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes==0.37.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/18/75dbd7529844c8600944df123160216323982d39d24a30e9f6806279f935/bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.17.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/ca/96a50d122bd07d06c66e20e6bd275b5c8829602398f4e141f8755a25e31e/accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m559.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20.1,>=3.19.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/88/88/cd55f87e896b82a3aba8e6c0affc077de51f7321cf730622b17ef7b0f69c/protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.27.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/9b/2f536f9e73390209e0b27b74691355dac494b7ec8154f3012fdc6debbae7/transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting icetk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bf/8a/731927e0901273815b779e6ce0e081a95ecf78835ff80be30830505ae06c/icetk-0.0.7-py3-none-any.whl (16 kB)\n",
      "Collecting cpm_kernels==1.0.11\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/af/84/1831ce6ffa87b8fd4d9673c3595d0fc4e6631c0691eb43f406d3bf89b951/cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.13.1+cu117)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.2.0)\n",
      "Collecting datasets==2.10.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/17/5825fdf034ff1a315becdbb9b6fe5a2bd9d8e724464535f18809593bf9c2/datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (23.0)\n",
      "Collecting filelock\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9e/6b/fdcd53aeee771a868c4187f0955116894a2b1e82d73fb5990c2ef63afc18/filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/df/90/5ad98abead047169f4f86bc67e99020c841d71c9c6bd202e04af71e70e53/huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (4.64.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/74/71/abf5df0be7a29b6920d4ae85eb685584afbe84610631b70fe366b2857801/regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4e/f2/017bf57106b845e31ef6179bf204042720a53629cf599ef9464da990e0e5/tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2.28.1)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/91/708bcf6e636fc4f1a07bdb704c0a320bafe9b83919cd501648307b31f555/pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 14)) (1.5.2)\n",
      "Collecting multiprocess\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/13/95/8b875a678c6f9db81809dd5d6032e9f8628426e37f6aa6b7d404ba582de1/multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4f/65/887925f1549fcb6ac3abb23a747c10f5ab083e8471fe568768b18bdb15b2/fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1a/d7/a42f83d34d4999321e06ca273f5e7bf7fa177154e29e0bfe455f3c66648d/xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d2/e5/cef5eeb11d7e8bac830b3bee1c8311b19bf8e8a1c45fe14b876c70adcd06/aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0e/7e/a69d054029c7c0470e490b3265bbd1497df9492599b1820b9d5be2c60444/sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting icetk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ca/eb/db3b8d7e891a959bd53641019f7b7e0ece6bfe9d89a6316d011bb6e0afd2/icetk-0.0.6-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/site-packages (from icetk->-r requirements.txt (line 8)) (0.14.1+cu117)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1e/21/4dc97f0ffc0b833dd3bf667e214e65f98fc361af56a82b34039383a9e05c/icetk-0.0.5-py3-none-any.whl (15 kB)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/c6/1fe059fff5d532122b5a93be15b23bc9eedb5e0eda24d51ae9e389584f17/icetk-0.0.4-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.8.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.35.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (65.6.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.51.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 11)) (1.16.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/ab/a440db757401a1e8863c9abb374a77cb2884eda74ffbf555dedcf1fbe7f6/frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/0d/0ba1f2022b9a36ae670c1f3c579ed08d0958398cb6beaf4687e606ad33d4/yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/0c/8469202f8f4b0e65816f91c3febc4bda7316c995b59ecdf3b15c574f7a24/multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m373.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 14)) (22.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 11)) (6.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 14)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 14)) (2022.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/site-packages (from torchvision->icetk->-r requirements.txt (line 8)) (9.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 11)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 11)) (3.2.2)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=41645 sha256=fd508a7f91ad6d394157d40f27ea170ed58b6c0d00c3a96223c25dfe95d3add0\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/70/31/9924c81a28479a21d715f7bfae13723f624e706d93b97c4b78\n",
      "Successfully built peft\n",
      "Installing collected packages: tokenizers, sentencepiece, cpm_kernels, bitsandbytes, xxhash, regex, pyarrow, protobuf, multidict, fsspec, frozenlist, filelock, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, accelerate, transformers, icetk, aiohttp, peft, datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.17.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.37.1 cpm_kernels-1.0.11 datasets-2.10.1 dill-0.3.6 filelock-3.11.0 frozenlist-1.3.3 fsspec-2023.3.0 huggingface-hub-0.13.4 icetk-0.0.4 multidict-6.0.4 multiprocess-0.70.14 peft-0.3.0.dev0 protobuf-3.20.0 pyarrow-11.0.0 regex-2023.3.23 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.3 transformers-4.27.1 xxhash-3.2.0 yarl-1.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec15008-542b-4f49-a7bf-7e3c43db3fbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 准备数据\n",
    "## cover_alpaca2jsonl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a296d5-38b0-4dff-95b4-83c0610ec42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: 创建用户故事 \\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(\"/openbayes/input/input0/datasets/userstory_detail.jsonl\") as f:\n",
    "        examples = list(f)\n",
    "\n",
    "    with open(\"/output/train.jsonl\", 'w') as f:\n",
    "        for example in tqdm(examples, desc=\"formatting..\"):\n",
    "            f.write(json.dumps(format_example(json.loads(example))) + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf40bb-8bb0-4fcb-bd14-9b414eb32426",
   "metadata": {},
   "source": [
    "## tokenize_dataset_rows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853b76a-3775-4cbc-b8d6-dd3157cc844a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "\n",
    "def preprocess(tokenizer, config, example, max_seq_length):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True,return_attention_mask=False,\n",
    "                add_special_tokens=False)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_attention_mask=False,\n",
    "        add_special_tokens=False)\n",
    "    input_ids = prompt_ids + [150001, 150004] + target_ids + [150005]\n",
    "    return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "\n",
    "\n",
    "def read_jsonl(path, max_seq_length, skip_overlength=False):\n",
    "    model_name = \"/openbayes/input/input1\"\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name, trust_remote_code=True)\n",
    "    config = transformers.AutoConfig.from_pretrained(\n",
    "        model_name, trust_remote_code=True, device_map='auto')\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            example = json.loads(line)\n",
    "            feature = preprocess(tokenizer, config, example, max_seq_length)\n",
    "            if skip_overlength and len(feature[\"input_ids\"]) > max_seq_length:\n",
    "                continue\n",
    "            feature[\"input_ids\"] = feature[\"input_ids\"][:max_seq_length]\n",
    "            yield feature\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = datasets.Dataset.from_generator(\n",
    "        lambda: read_jsonl(\"/output/train.jsonl\", 384, False)\n",
    "    )\n",
    "    dataset.save_to_disk(\"/output/train\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc6e27-1e1f-4bd8-9007-03bcaa66cfef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a47ea-3edb-47b6-acb7-676eaeece34a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 训练前推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbc632-542f-42e6-8b08-1ba36135b5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/openbayes/input/input1\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75c23f-505a-40e8-aacc-4c6629d22214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec68f27-f503-4295-b91d-3043230ed1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: 创建用户故事\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b63873-623b-4b7b-96af-0327583c383e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/openbayes/input/input0/datasets/userstory_detail.jsonl\") as f:\n",
    "    examples = list(f)\n",
    "\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    for example in examples[:5]:\n",
    "        item = json.loads(example)\n",
    "        feature = format_example(item)\n",
    "        input_text = feature['context']\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "        for k,v in inputs.items():\n",
    "            if v is not None:\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "        out = outputs[0].tolist()[input_ids.size()[-1]:]\n",
    "        answer = tokenizer.decode(out)\n",
    "        item['infer_answer'] = answer\n",
    "        print(input_text)\n",
    "        print(answer)\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b5290-4037-4307-994a-f18e4dd8cede",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec408aa-4621-42a5-9ff7-81da8cf284f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T05:07:49.998319Z",
     "iopub.status.busy": "2023-04-08T05:07:49.997879Z",
     "iopub.status.idle": "2023-04-08T06:21:51.214643Z",
     "shell.execute_reply": "2023-04-08T06:21:51.213181Z",
     "shell.execute_reply.started": "2023-04-08T05:07:49.998279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from transformers.integrations import TensorBoardCallback\n",
      "from torch.utils.tensorboard import SummaryWriter\n",
      "from transformers import TrainingArguments\n",
      "from transformers import Trainer, HfArgumentParser\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from peft import get_peft_model, LoraConfig, TaskType\n",
      "from dataclasses import dataclass, field\n",
      "import datasets\n",
      "import os\n",
      "\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class FinetuneArguments:\n",
      "    dataset_path: str = field(default=\"data/alpaca\")\n",
      "    model_path: str = field(default=\"output\")\n",
      "    lora_rank: int = field(default=8)\n",
      "\n",
      "\n",
      "class CastOutputToFloat(nn.Sequential):\n",
      "    def forward(self, x):\n",
      "        return super().forward(x).to(torch.float32)\n",
      "\n",
      "\n",
      "def data_collator(features: list) -> dict:\n",
      "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
      "    longest = max(len_ids)\n",
      "    input_ids = []\n",
      "    labels_list = []\n",
      "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
      "        ids = feature[\"input_ids\"]\n",
      "        seq_len = feature[\"seq_len\"]\n",
      "        labels = (\n",
      "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
      "        )\n",
      "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
      "        _ids = torch.LongTensor(ids)\n",
      "        labels_list.append(torch.LongTensor(labels))\n",
      "        input_ids.append(_ids)\n",
      "    input_ids = torch.stack(input_ids)\n",
      "    labels = torch.stack(labels_list)\n",
      "    return {\n",
      "        \"input_ids\": input_ids,\n",
      "        \"labels\": labels,\n",
      "    }\n",
      "\n",
      "\n",
      "class ModifiedTrainer(Trainer):\n",
      "    def compute_loss(self, model, inputs, return_outputs=False):\n",
      "        return model(\n",
      "            input_ids=inputs[\"input_ids\"],\n",
      "            labels=inputs[\"labels\"],\n",
      "        ).loss\n",
      "\n",
      "    def save_model(self, output_dir=None, _internal_call=False):\n",
      "        from transformers.trainer import TRAINING_ARGS_NAME\n",
      "\n",
      "        os.makedirs(output_dir, exist_ok=True)\n",
      "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
      "        saved_params = {\n",
      "            k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
      "        }\n",
      "        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))\n",
      "\n",
      "\n",
      "def main():\n",
      "    writer = SummaryWriter()\n",
      "    finetune_args, training_args = HfArgumentParser(\n",
      "        (FinetuneArguments, TrainingArguments)\n",
      "    ).parse_args_into_dataclasses()\n",
      "\n",
      "    # init model\n",
      "    model = AutoModel.from_pretrained(\n",
      "        \"/openbayes/input/input1\", load_in_8bit=True, trust_remote_code=True, device_map=\"auto\"\n",
      "    )\n",
      "    model.gradient_checkpointing_enable()\n",
      "    model.enable_input_require_grads()\n",
      "    model.is_parallelizable = True\n",
      "    model.model_parallel = True\n",
      "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
      "    model.config.use_cache = (\n",
      "        False  # silence the warnings. Please re-enable for inference!\n",
      "    )\n",
      "\n",
      "    # setup peft\n",
      "    peft_config = LoraConfig(\n",
      "        task_type=TaskType.CAUSAL_LM,\n",
      "        inference_mode=False,\n",
      "        r=finetune_args.lora_rank,\n",
      "        lora_alpha=32,\n",
      "        lora_dropout=0.1,\n",
      "    )\n",
      "    model = get_peft_model(model, peft_config)\n",
      "\n",
      "    # load dataset\n",
      "    dataset = datasets.load_from_disk(finetune_args.dataset_path)\n",
      "    print(f\"\\n{len(dataset)=}\\n\")\n",
      "\n",
      "    # start train\n",
      "    trainer = ModifiedTrainer(\n",
      "        model=model,\n",
      "        train_dataset=dataset,\n",
      "        args=training_args,\n",
      "        callbacks=[TensorBoardCallback(writer)],\n",
      "        data_collator=data_collator,\n",
      "    )\n",
      "    trainer.train()\n",
      "    writer.close()\n",
      "    # save model\n",
      "    model.save_pretrained(training_args.output_dir)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('home/51a6a779-f6e2-4d8f-970a-4d47e7ace1bf')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('tcp'), PosixPath('//10.111.0.1'), PosixPath('443')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('7890'), PosixPath('//alchemist-experience'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('localhost,127.0.0.1,openbayes-server-svc,openbayes-storage-server-svc,10.0.0.0/8')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('phodal/jobs/utn64jwm5lct')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/output/.torch')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//openbayes-server-svc/api/users/phodal/jobs/utn64jwm5lct'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:15<00:00,  1.95s/it]\n",
      "\n",
      "len(dataset)=3438\n",
      "\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "/usr/local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 1.662, 'learning_rate': 9.84e-05, 'epoch': 0.09}                       \n",
      "{'loss': 0.5857, 'learning_rate': 9.673333333333334e-05, 'epoch': 0.17}         \n",
      "{'loss': 0.5188, 'learning_rate': 9.506666666666667e-05, 'epoch': 0.26}         \n",
      "{'loss': 0.492, 'learning_rate': 9.340000000000001e-05, 'epoch': 0.35}          \n",
      "{'loss': 0.4631, 'learning_rate': 9.173333333333333e-05, 'epoch': 0.44}         \n",
      "{'loss': 0.43, 'learning_rate': 9.006666666666667e-05, 'epoch': 0.52}           \n",
      "{'loss': 0.416, 'learning_rate': 8.840000000000001e-05, 'epoch': 0.61}          \n",
      "{'loss': 0.4269, 'learning_rate': 8.673333333333333e-05, 'epoch': 0.7}          \n",
      "{'loss': 0.4113, 'learning_rate': 8.506666666666667e-05, 'epoch': 0.79}         \n",
      "{'loss': 0.3968, 'learning_rate': 8.34e-05, 'epoch': 0.87}                      \n",
      "{'loss': 0.411, 'learning_rate': 8.173333333333335e-05, 'epoch': 0.96}          \n",
      "{'loss': 0.3767, 'learning_rate': 8.006666666666667e-05, 'epoch': 1.05}         \n",
      "{'loss': 0.3651, 'learning_rate': 7.840000000000001e-05, 'epoch': 1.13}         \n",
      "{'loss': 0.362, 'learning_rate': 7.673333333333333e-05, 'epoch': 1.22}          \n",
      "{'loss': 0.3591, 'learning_rate': 7.506666666666667e-05, 'epoch': 1.31}         \n",
      "{'loss': 0.3598, 'learning_rate': 7.340000000000001e-05, 'epoch': 1.4}          \n",
      "{'loss': 0.3498, 'learning_rate': 7.173333333333335e-05, 'epoch': 1.48}         \n",
      "{'loss': 0.3359, 'learning_rate': 7.006666666666667e-05, 'epoch': 1.57}         \n",
      "{'loss': 0.3503, 'learning_rate': 6.840000000000001e-05, 'epoch': 1.66}         \n",
      "{'loss': 0.3396, 'learning_rate': 6.673333333333333e-05, 'epoch': 1.75}         \n",
      " 33%|█████████████                          | 1000/3000 [24:27<48:37,  1.46s/it]/usr/local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.3462, 'learning_rate': 6.506666666666666e-05, 'epoch': 1.83}         \n",
      "{'loss': 0.3039, 'learning_rate': 6.340000000000001e-05, 'epoch': 1.92}         \n",
      "{'loss': 0.3226, 'learning_rate': 6.173333333333333e-05, 'epoch': 2.01}         \n",
      "{'loss': 0.2848, 'learning_rate': 6.006666666666667e-05, 'epoch': 2.09}         \n",
      "{'loss': 0.292, 'learning_rate': 5.8399999999999997e-05, 'epoch': 2.18}         \n",
      "{'loss': 0.3001, 'learning_rate': 5.673333333333334e-05, 'epoch': 2.27}         \n",
      "{'loss': 0.2849, 'learning_rate': 5.5066666666666666e-05, 'epoch': 2.36}        \n",
      "{'loss': 0.2927, 'learning_rate': 5.3400000000000004e-05, 'epoch': 2.44}        \n",
      "{'loss': 0.2988, 'learning_rate': 5.1733333333333335e-05, 'epoch': 2.53}        \n",
      "{'loss': 0.3075, 'learning_rate': 5.006666666666667e-05, 'epoch': 2.62}         \n",
      "{'loss': 0.289, 'learning_rate': 4.8400000000000004e-05, 'epoch': 2.71}         \n",
      "{'loss': 0.2884, 'learning_rate': 4.6733333333333335e-05, 'epoch': 2.79}        \n",
      "{'loss': 0.2697, 'learning_rate': 4.5066666666666667e-05, 'epoch': 2.88}        \n",
      "{'loss': 0.3035, 'learning_rate': 4.3400000000000005e-05, 'epoch': 2.97}        \n",
      "{'loss': 0.2725, 'learning_rate': 4.1733333333333336e-05, 'epoch': 3.05}        \n",
      "{'loss': 0.2522, 'learning_rate': 4.006666666666667e-05, 'epoch': 3.14}         \n",
      "{'loss': 0.2477, 'learning_rate': 3.8400000000000005e-05, 'epoch': 3.23}        \n",
      "{'loss': 0.2604, 'learning_rate': 3.6733333333333336e-05, 'epoch': 3.32}        \n",
      "{'loss': 0.2622, 'learning_rate': 3.506666666666667e-05, 'epoch': 3.4}          \n",
      "{'loss': 0.2387, 'learning_rate': 3.3400000000000005e-05, 'epoch': 3.49}        \n",
      " 67%|██████████████████████████             | 2000/3000 [49:00<22:30,  1.35s/it]/usr/local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "{'loss': 0.2547, 'learning_rate': 3.173333333333334e-05, 'epoch': 3.58}         \n",
      "{'loss': 0.2603, 'learning_rate': 3.006666666666667e-05, 'epoch': 3.66}         \n",
      "{'loss': 0.2403, 'learning_rate': 2.84e-05, 'epoch': 3.75}                      \n",
      "{'loss': 0.261, 'learning_rate': 2.6733333333333334e-05, 'epoch': 3.84}         \n",
      "{'loss': 0.2488, 'learning_rate': 2.5066666666666665e-05, 'epoch': 3.93}        \n",
      "{'loss': 0.2493, 'learning_rate': 2.3400000000000003e-05, 'epoch': 4.01}        \n",
      "{'loss': 0.2279, 'learning_rate': 2.1733333333333334e-05, 'epoch': 4.1}         \n",
      "{'loss': 0.23, 'learning_rate': 2.0066666666666665e-05, 'epoch': 4.19}          \n",
      "{'loss': 0.235, 'learning_rate': 1.84e-05, 'epoch': 4.28}                       \n",
      "{'loss': 0.2319, 'learning_rate': 1.6733333333333335e-05, 'epoch': 4.36}        \n",
      "{'loss': 0.2278, 'learning_rate': 1.5066666666666668e-05, 'epoch': 4.45}        \n",
      "{'loss': 0.2227, 'learning_rate': 1.3400000000000002e-05, 'epoch': 4.54}        \n",
      "{'loss': 0.2238, 'learning_rate': 1.1733333333333333e-05, 'epoch': 4.62}        \n",
      "{'loss': 0.2179, 'learning_rate': 1.0066666666666668e-05, 'epoch': 4.71}        \n",
      "{'loss': 0.2166, 'learning_rate': 8.400000000000001e-06, 'epoch': 4.8}          \n",
      "{'loss': 0.229, 'learning_rate': 6.733333333333333e-06, 'epoch': 4.89}          \n",
      "{'loss': 0.2272, 'learning_rate': 5.066666666666667e-06, 'epoch': 4.97}         \n",
      "{'loss': 0.2164, 'learning_rate': 3.4000000000000005e-06, 'epoch': 5.06}        \n",
      "{'loss': 0.2134, 'learning_rate': 1.7333333333333334e-06, 'epoch': 5.15}        \n",
      "{'loss': 0.199, 'learning_rate': 6.666666666666667e-08, 'epoch': 5.24}          \n",
      "{'train_runtime': 4414.8872, 'train_samples_per_second': 4.077, 'train_steps_per_second': 0.68, 'train_loss': 0.329377610206604, 'epoch': 5.24}\n",
      "100%|█████████████████████████████████████| 3000/3000 [1:13:34<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "!sed -i \"s/THUDM\\/chatglm-6b/\\/openbayes\\/input\\/input1/\" finetune.py\n",
    "!cat finetune.py\n",
    "!python finetune.py \\\n",
    "    --dataset_path /output/train \\\n",
    "    --lora_rank 8 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --max_steps 3000 \\\n",
    "    --save_steps 1000 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --fp16 \\\n",
    "    --remove_unused_columns false \\\n",
    "    --logging_steps 50 \\\n",
    "    --output_dir /output/lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35a70e-3ff4-4313-80e6-2d73d2b148da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练后推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37829bbb-299f-4fc7-b9d9-44ed9d5b339c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:29:07.082415Z",
     "iopub.status.busy": "2023-04-08T06:29:07.081659Z",
     "iopub.status.idle": "2023-04-08T06:29:17.168455Z",
     "shell.execute_reply": "2023-04-08T06:29:17.167487Z",
     "shell.execute_reply.started": "2023-04-08T06:29:07.082377Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.111.0.1'), PosixPath('tcp'), PosixPath('443')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//alchemist-experience'), PosixPath('7890'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('localhost,127.0.0.1,openbayes-server-svc,openbayes-storage-server-svc,10.0.0.0/8')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('phodal/jobs/utn64jwm5lct')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/output/.torch')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//openbayes-server-svc/api/users/phodal/jobs/utn64jwm5lct'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('home/51a6a779-f6e2-4d8f-970a-4d47e7ace1bf')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627bd5a4f4c44850895fc4bfbe0ec0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/usr/local/lib/python3.8/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = AutoModel.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/openbayes/input/input1\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "peft_path = \"/output/lora/adapter_model.bin\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=True,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40560b-5393-4e95-be72-fd9e94e968c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 原输入测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2e2d4c-8bf6-4f7f-baa5-3cf8cc4f242d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:30:00.576300Z",
     "iopub.status.busy": "2023-04-08T06:30:00.575974Z",
     "iopub.status.idle": "2023-04-08T06:30:23.957432Z",
     "shell.execute_reply": "2023-04-08T06:30:23.956915Z",
     "shell.execute_reply.started": "2023-04-08T06:30:00.576282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: create user story tasks \n",
      "Input:  Animation and Comics:Browse and search for animations and comics\n",
      "Answer: \n",
      "\n",
      "用户故事:可以浏览和搜索动画和漫画\n",
      "作为一个动画和漫画爱好者\n",
      "我想在动画和漫画应用中浏览和搜索动画和漫画\n",
      "以便于我能够找到我最喜欢的动画和漫画\n",
      "\n",
      "AC 1: 动画和漫画爱好者可以在动画和漫画应用中浏览和搜索动画和漫画\n",
      "假设 用户已经登录了动画和漫画应用\n",
      "当 用户点击浏览和搜索按钮\n",
      "于是 用户可以看到所有动画和漫画的列表,并可以搜索特定的动画和漫画\n",
      "### 1.Answer:\n",
      " \n",
      "用户故事：可以浏览和搜索动画和漫画\n",
      "作为一个 Animation and Comics 应用的用户\n",
      "我想要浏览和搜索动画和漫画\n",
      "以便于我能够找到我喜欢的动画和漫画\n",
      "\n",
      "AC 1: 用户可以浏览和搜索动画和漫画\n",
      "假设 用户打开 Animation and Comics 应用\n",
      "当 用户点击浏览和搜索动画和漫画按钮\n",
      "于是 用户可以看到动画和漫画的列表，并可以搜索特定的动画和漫画 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks \n",
      "Input:  Animation and Comics:Participate in online forums and discussions\n",
      "Answer: \n",
      "\n",
      "用户故事:参与在线论坛和讨论\n",
      "作为一个Animation and Comics应用的用户\n",
      "我想参与在线论坛和讨论\n",
      "以便于我可以与其他用户分享我的观点和经验,并且可以获取其他用户的观点和经验。\n",
      "\n",
      "AC 1: 用户可以参与在线论坛和讨论\n",
      "假设 用户已经登录Animation and Comics应用\n",
      "当 用户点击参与在线论坛和讨论按钮\n",
      "于是 用户可以参与在线论坛和讨论\n",
      "### 2.Answer:\n",
      " \n",
      "用户故事：可以参与在线论坛和讨论\n",
      "作为一个 Animation and Comics 应用的用户\n",
      "我想参与在线论坛和讨论\n",
      "以便于我可以与其他爱好者分享我的想法，观点和经验。\n",
      "\n",
      "AC 1: 用户可以参与在线论坛和讨论\n",
      "假设 用户已经登录到 Animation and Comics 应用\n",
      "当 用户点击论坛按钮\n",
      "于是 用户可以参与在线论坛和讨论 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks \n",
      "Input:  Jobs and Career:Follow companies\n",
      "Answer: \n",
      "\n",
      "用户故事:可以关注公司\n",
      "作为一个求职者\n",
      "我想在 Jobs and Career 应用中关注公司\n",
      "以便于我可以及时了解这些公司的招聘信息\n",
      "\n",
      "AC 1: 求职者可以在 Jobs and Career 应用中关注公司\n",
      "假设 求职者已经登录了 Jobs and Career 应用\n",
      "当 求职者点击 Follow companies 按钮\n",
      "于是 求职者可以关注公司,并且可以及时了解这些公司的招聘信息\n",
      "### 3.Answer:\n",
      " \n",
      "用户故事：可以关注公司\n",
      "作为一个求职者\n",
      "我想在Jobs and Career应用中关注我感兴趣的公司\n",
      "以便于我可以收到有关这些公司的最新招聘信息\n",
      "\n",
      "AC 1: 求职者可以在Jobs and Career应用中关注公司\n",
      "假设 求职者已经登录了Jobs and Career应用\n",
      "当 求职者点击“Follow companies”按钮\n",
      "于是 求职者可以搜索并关注感兴趣的公司\n",
      "\n",
      "AC 2: 求职者可以收到关注公司的最新招聘信息\n",
      "假设 求职者已经关注了多个公司\n",
      "当 这些公司发布了新的招聘信息\n",
      "于是 求职者可以收到有关这些公司的最新招聘信息 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks \n",
      "Input:  Basketball:View team tournaments\n",
      "Answer: \n",
      "\n",
      "用户故事:可以查看篮球队的比赛\n",
      "作为一个篮球爱好者\n",
      "我想在篮球应用中查看篮球队的比赛\n",
      "以便于我可以查看篮球队的比赛信息,如比赛时间、地点、对手等\n",
      "\n",
      "AC 1: 篮球爱好者可以查看篮球队的比赛\n",
      "假设 篮球爱好者打开篮球应用\n",
      "当 篮球爱好者点击“查看篮球队的比赛”按钮\n",
      "于是 篮球爱好者可以查看篮球队的比赛信息,如比赛时间、地点、对手等\n",
      "### 4.Answer:\n",
      " \n",
      "用户故事：可以查看篮球队的比赛\n",
      "作为一个篮球爱好者\n",
      "我想在篮球应用中查看篮球队的比赛\n",
      "以便于我可以查看篮球队的比赛信息，如比赛时间、地点、对手等\n",
      "\n",
      "AC 1: 篮球爱好者可以查看篮球队的比赛\n",
      "假设 篮球爱好者打开篮球应用\n",
      "当 篮球爱好者点击“View team tournaments”按钮\n",
      "于是 篮球爱好者可以查看篮球队的比赛信息，如比赛时间、地点、对手等 \n",
      "\n",
      "\n",
      "Instruction: create user story tasks \n",
      "Input:  Basketball:View team clinics\n",
      "Answer: \n",
      "\n",
      "用户故事:可以查看篮球队诊所\n",
      "作为一个篮球爱好者\n",
      "我想在篮球应用中查看篮球队诊所\n",
      "以便于我可以查看篮球队的诊所信息,如地址,联系方式,开放时间等\n",
      "\n",
      "AC 1: 篮球爱好者可以查看篮球队诊所\n",
      "假设 篮球爱好者打开篮球应用\n",
      "当 篮球爱好者点击查看篮球队诊所\n",
      "于是 篮球爱好者可以查看篮球队诊所的信息,如地址,联系方式,开放时间等\n",
      "### 5.Answer:\n",
      " \n",
      "用户故事：可以查看篮球队诊所\n",
      "作为一个篮球爱好者\n",
      "我想在篮球应用中查看篮球队诊所\n",
      "以便于我可以查看篮球队的诊所信息，如地址，联系方式，开放时间等\n",
      "\n",
      "AC 1: 篮球爱好者可以查看篮球队诊所\n",
      "假设 篮球爱好者打开篮球应用\n",
      "当 篮球爱好者点击查看篮球队诊所\n",
      "于是 篮球爱好者可以查看篮球队诊所的信息，如地址，联系方式，开放时间等 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: create user story tasks \\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "with open(\"/openbayes/input/input0/datasets/userstory_detail.jsonl\") as f:\n",
    "    examples = list(f)\n",
    "\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    for example in examples[:5]:\n",
    "        item = json.loads(example)\n",
    "        feature = format_example(item)\n",
    "        input_text = feature['context']\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "        for k,v in inputs.items():\n",
    "            if v is not None:\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "        out = outputs[0].tolist()[input_ids.size()[-1]:]\n",
    "        answer = tokenizer.decode(out)\n",
    "        item['infer_answer'] = answer\n",
    "        print(input_text)\n",
    "        print(answer)\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8471d07-54fd-4d90-823e-1b84c9009294",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 任意Prompt推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bf6699-8956-4b13-a265-2b0ab846b46e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:30:28.828403Z",
     "iopub.status.busy": "2023-04-08T06:30:28.828068Z",
     "iopub.status.idle": "2023-04-08T06:30:28.832450Z",
     "shell.execute_reply": "2023-04-08T06:30:28.832056Z",
     "shell.execute_reply.started": "2023-04-08T06:30:28.828384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    inputs = model.prepare_inputs_for_generation(input_ids)\n",
    "    for k,v in inputs.items():\n",
    "        if v is not None:\n",
    "            inputs[k] = v.to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)\n",
    "    out = outputs[0].tolist()[input_ids.size()[-1]:]\n",
    "    answer = tokenizer.decode(out)\n",
    "\n",
    "    print(input_text)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a02b2c-7e7f-47ce-a0a6-42c057c3517d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:35:31.278376Z",
     "iopub.status.busy": "2023-04-08T06:35:31.277932Z",
     "iopub.status.idle": "2023-04-08T06:35:38.123368Z",
     "shell.execute_reply": "2023-04-08T06:35:38.122851Z",
     "shell.execute_reply.started": "2023-04-08T06:35:31.278358Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: create Agile user story for following topic \n",
      "Input: 美团外卖：注册外卖骑士\n",
      "美团外卖:注册外卖骑士\n",
      "\n",
      "作为 美团外卖的用户\n",
      "我想 注册一个外卖骑士\n",
      "以便于 我可以使用外卖骑士的功能\n",
      "\n",
      "AC 1: 美团外卖的用户可以使用注册表单注册外卖骑士\n",
      "假设 用户输入了注册表单中的所有信息\n",
      "当 用户提交注册表单\n",
      "于是 用户可以注册一个外卖骑士\n",
      "\n",
      "AC 2: 美团外卖的用户可以使用手机号码注册外卖骑士\n",
      "假设 用户输入了手机号码\n",
      "当 用户提交注册表单\n",
      "于是 用户可以注册一个外卖骑士\n",
      "\n",
      "AC 3: 美团外卖的用户可以使用邮箱地址注册外卖骑士\n",
      "假设 用户输入了邮箱地址\n",
      "当 用户提交注册表单\n",
      "于是 用户可以注册一个外卖骑士\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Instruction: create Agile user story for following topic \\nInput: 美团外卖：注册外卖骑士\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97ee5148-a6b5-4388-bad8-a315f66a7fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:40:36.303556Z",
     "iopub.status.busy": "2023-04-08T06:40:36.303171Z",
     "iopub.status.idle": "2023-04-08T06:40:42.404154Z",
     "shell.execute_reply": "2023-04-08T06:40:42.403640Z",
     "shell.execute_reply.started": "2023-04-08T06:40:36.303527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 创建用户故事\n",
      "Input: 电影网站: 查看订单详情 \n",
      "Answer:\n",
      "\n",
      "用户故事:可以查看订单详情\n",
      "作为一个电影网站的用户\n",
      "我想查看订单详情\n",
      "以便于我可以了解订单的详细信息,如订单号、订单状态、订单内容等。\n",
      "\n",
      "AC 1: 用户可以查看订单详情\n",
      "假设 用户已经登录\n",
      "当 用户点击查看订单详情按钮\n",
      "于是 用户可以查看订单详情,包括订单号、订单状态、订单内容等。\n",
      "\n",
      "AC 2: 用户可以查看订单详情\n",
      "假设 用户没有登录\n",
      "当 用户点击查看订单详情按钮\n",
      "于是 用户被要求先登录,然后才能查看订单详情,包括订单号、订单状态、订单内容等。\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Instruction: 创建用户故事\\nInput: 电影网站: 查看订单详情 \\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d575f044-e6df-4752-92bb-9e5a9b7bb2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:31:47.070624Z",
     "iopub.status.busy": "2023-04-08T06:31:47.070331Z",
     "iopub.status.idle": "2023-04-08T06:31:52.270944Z",
     "shell.execute_reply": "2023-04-08T06:31:52.270422Z",
     "shell.execute_reply.started": "2023-04-08T06:31:47.070601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 创建用户故事\n",
      "Input: 团购网站:用户注册\n",
      "Answer:\n",
      "团购网站的用户注册故事:\n",
      "\n",
      "作为一个团购网站的用户\n",
      "\n",
      "我想注册一个团购网站的用户账号\n",
      "\n",
      "以便于我可以使用团购服务,如购买商品、服务等。\n",
      "\n",
      "AC 1: 团购网站的用户注册账号\n",
      "假设 用户输入了注册信息\n",
      "当 用户提交注册信息\n",
      "于是 用户可以成功注册一个团购网站的用户账号\n",
      "\n",
      "AC 2: 团购网站的用户注册账号\n",
      "假设 用户输入了错误的注册信息\n",
      "当 用户提交注册信息\n",
      "于是 用户可以被提示输入正确的注册信息,并重新提交注册信息\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Instruction: 创建用户故事\\nInput: 团购网站:用户注册\\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf83312-6949-4284-ad75-50ad083de35c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:31:52.271946Z",
     "iopub.status.busy": "2023-04-08T06:31:52.271783Z",
     "iopub.status.idle": "2023-04-08T06:31:56.072283Z",
     "shell.execute_reply": "2023-04-08T06:31:56.071787Z",
     "shell.execute_reply.started": "2023-04-08T06:31:52.271931Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 创建用户故事\n",
      "Input: 博客网站:发表新文章\n",
      "Answer:\n",
      "\n",
      "\n",
      "用户故事:可以在博客网站上发表新文章\n",
      "作为一个博客作者\n",
      "我想在博客网站上发表新文章\n",
      "以便于我可以发布最新的博客内容,并且可以吸引更多的读者。\n",
      "\n",
      "AC 1: 博客作者可以在博客网站上发表新文章\n",
      "假设 博客作者已经登录博客网站\n",
      "当 博客作者点击发表新文章按钮\n",
      "于是 博客作者可以在博客网站上发表新文章\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Instruction: 创建用户故事\\nInput: 博客网站:发表新文章\\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ef11023-c3a7-4ae4-81d7-43eaed264065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:38:37.700745Z",
     "iopub.status.busy": "2023-04-08T06:38:37.700452Z",
     "iopub.status.idle": "2023-04-08T06:38:41.301980Z",
     "shell.execute_reply": "2023-04-08T06:38:41.301479Z",
     "shell.execute_reply.started": "2023-04-08T06:38:37.700727Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 创建用户故事\n",
      "Input: 京西商城：填写订单信息 \n",
      "Answer:\n",
      "用户故事:可以在京西商城填写订单信息\n",
      "作为一个京西商城的用户\n",
      "我想在购物时填写订单信息\n",
      "以便于我能够完成订单\n",
      "\n",
      "AC 1: 用户可以在京西商城填写订单信息\n",
      "假设 用户已经选择了商品\n",
      "当 用户点击提交订单按钮\n",
      "于是 用户可以在订单信息表单中填写订单信息,并提交订单\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Instruction: 创建用户故事\\nInput: 京西商城：填写订单信息 \\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61e2c8-b5b7-4949-bf27-926fa485b802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-08T06:33:05.605308Z",
     "iopub.status.busy": "2023-04-08T06:33:05.604871Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: 创建用户故事 \n",
      "Input: 巨信聊天：发送文本信息\n",
      "Answer:\n",
      "\n",
      "作为一个巨信聊天应用的用户\n",
      "我想要发送文本信息\n",
      "以便于我能够与朋友进行有效的沟通\n",
      "\n",
      "AC 1: 用户可以发送文本信息\n",
      "假设 用户已经登录巨信聊天应用\n",
      "当 用户点击发送按钮\n",
      "于是 用户可以选择发送文本信息\n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Instruction: 创建用户故事 \\nInput: 巨信聊天：发送文本信息\\nAnswer:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7ac53-77cd-4070-acbc-82c39e175328",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"Instruction: 创建用户故事 \\nInput: 巨信聊天：发送文本信息\\nAnswer:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
